# -*- coding: utf-8 -*-
"""6959Tuplex.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cxhOZzjfXvZKBgmUYlZuFcKr32Y-16C0

##Package setup
"""

import pandas as pd
import numpy as np
import csv

# visualization for categorical cols
import seaborn as sns
import matplotlib.pyplot as plt

# ML
from sklearn.preprocessing import LabelEncoder # encode categorical labels
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler # standardize the nurmerical cols

from sklearn.model_selection import train_test_split

from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier

from sklearn.metrics import accuracy_score, classification_report

# install Colab compatible upgrades to avoid dependency errors
!pip install -q folium==0.2.1
!pip install -q --upgrade urllib3==1.25.11
!pip install -q flask-socketio flask-pymongo eventlet==0.30.0
!pip uninstall -q jedi -y && pip3 install 'jedi>=0.10'

# install Tuplex
!pip install -q tuplex

import tuplex

c = tuplex.Context({'tuplex.redirectToPythonLogging':False})

"""
Read Dataset
"""
# GitHub URL of the raw dataset file
income_url = 'https://raw.githubusercontent.com/Ashley00/6959tuplex/main/income.csv'

# Read the dataset into a pandas DataFrame
income_original = pd.read_csv(income_url)

# get a copy of datasets
income = income_original.copy()

"""## Data analysis"""

# get features
features = list(income.columns)
# get each column type
types = income.dtypes

print(types)

income.describe(include='all')

# handle categorical data and numerical
# feature col
num_cols = []
cate_cols = []
for col,t in income.dtypes.items():
  if t == 'object':
    cate_cols.append(col)
  else:
    if col != 'income>50K':
      num_cols.append(col)

"""## Data Visualization"""

# numerical col
num_plots = len(num_cols)
num_rows = (num_plots + 2) // 3  # Calculate the number of rows needed

fig, axs = plt.subplots(num_rows, 3, figsize=(15, 5*num_rows))  # Adjust figsize as needed

for i, col_name in enumerate(num_cols):
    row = i // 3
    col = i % 3
    axs[row, col].hist(income[col_name], color = "lightblue")
    axs[row, col].set_title("Histogram for " + col_name)
    axs[row, col].set_xlabel(col_name)
    axs[row, col].set_ylabel("Frequency")

# Remove any unused subplots
for i in range(num_plots, num_rows * 3):
    fig.delaxes(axs.flatten()[i])

plt.tight_layout()  # Adjust spacing between subplots
plt.show()

# categorical col

fig, axes = plt.subplots(4, 2, figsize=(24, 20))

for i, col in enumerate(cate_cols):
  data = income[[col]]
  ax = axes[i//2, i%2]  # Convert linear index to 2D index
  sns.countplot(x=col, data=data, ax=ax, color = "lightblue")
  ax.set_title(f'Count by {col}')

  ax.tick_params(axis='x', rotation=45) # Rotate labels to 45 degrees, so the labels don't overlap

  if i == 7:
    # Set the tick labels size
    tick_label_size = 6
    for label in (ax.get_xticklabels()):
      label.set_fontsize(tick_label_size)

plt.tight_layout()
plt.show()

"""## Data Cleaning and Feature Engineering -- **Tuplex**"""

# age: outliner + missing, outliner -> delete, missing -> median
# captial gain: negtive -> 0
# hour per week: 180 -> max
# fnlwgt: missing -> mean

# workclass: federal-gov, never-work, withou pay -> others
# country excep us, mexico, ? -> others

# check missing values
for col in income.columns:
  print("----------------")
  print("column name: %s count: %i, missing values: %i" % (col, income[col].count(), income[col].isnull().sum()))
  print("----------------")

#ds = c.csv('https://raw.githubusercontent.com/Ashley00/6959tuplex/main/income.csv')
# income.to_csv('income.csv', index = False, na_rep = 'None') # ??
ds = c.csv('income.csv') # load the data into colab

ds.show(5)

# numerical

# age outliner + missing, outliner -> delete, missing -> median
# captial gain negtive -> 0
# hour per week, 180 -> max
# fnlwgt missing -> mean

# calculate median age lees than 90
dataAge = ds.selectColumns(['age']).filter(lambda x : x['age'] is not None and x['age'] < 90).unique().collect()
dataAge = sorted(dataAge, key = lambda x: -x)
ageMedian = dataAge[int(len(dataAge)/2)]

# calculate max hour per week less than 180
dataHour = ds.filter(lambda x : x['hours.per.week'] > 90 and x['hours.per.week'] < 180).collect()
dataHour = sorted(dataHour, key=lambda x: -x[12]) # max 99
maxHourPerWeek = dataHour[:1][0][12]

# calculate mean fnlwgt
dataFnlwgt = ds.selectColumns(['fnlwgt']).filter(lambda x: x['fnlwgt'] is not None).collect()
meanFnlwgt = round(sum(dataFnlwgt)/len(dataFnlwgt)) # int

# define UDF to process numerical col

# age
def fillnaAge(row):
  age = row['age']
  if age is None:
    age = ageMedian # median
  return age

# capital.gain
def negativeCaptialGain(row):
  gain = row['capital.gain']
  if gain < 0:
    gain = 0
  return gain

# hours.per.week
def hourPerWeek(row):
  hour = row['hours.per.week']
  if hour > 24*7:
    hour = maxHourPerWeek # max
  return hour

# fnlwgt
def fillnaFnlwgt(row):
  fnlwgt = row['fnlwgt']
  if fnlwgt is None:
    fnlwgt = meanFnlwgt
  return fnlwgt

# remove outliners which age is larger than 90
# fill the missing values in age with median
# change the outliners of capital.gain which is negative to 0
# change the outliners of hours.per.week which is larger than 24*7 to the max hour after filtering out values larger than 24*7
# fill the missing values in fnlwgt with mean

ds_numDone = ds.filter(lambda x: x['age'] < 90)\
  .withColumn('age', fillnaAge)\
  .withColumn('capital.gain', negativeCaptialGain)\
  .withColumn('hours.per.week', hourPerWeek)\
  .withColumn('fnlwgt', fillnaFnlwgt)

# check whether the transformation is successful

ds.filter(lambda x: x['age'] == 90).show()

ds_numDone.filter(lambda x: x['age'] == 90).show()

# categorical

# workclass federal-gov, never-work, withou pay -> others
# country excep us, mexico, ? -> others

# get native.country list
dataConutry = ds_numDone.selectColumns(['native.country']).unique().collect()
dataConutry = [item for item in dataConutry if item not in ['United-States', 'Mexico', '?']]

# define UDF to process numerical col

# workclass
def workclassToOther(row):
  workclass = row['workclass']
  # if workclass in ['Federal-gov', 'Never-worked', 'Without-pay']: # don't work
  #   workclass = 'others'
  # for item in ['Federal-gov', 'Never-worked', 'Without-pay']: # don't work
  #   if item in workclass:
  #     workclass = 'others'
  if 'Federal-gov' in workclass or 'Never-worked' in workclass or 'Without-pay' in workclass:
    workclass = 'others'
  return workclass

# native.country
def countryToOther(row):
  country = row['native.country']
  # if 'United-States'in country:
  #   country = 'others'
  # if country in dataConutry: # don't work
  #   country = 'others'
  if 'United-States' in country or 'Mexico' in country or '?' in country:
    return country
  return 'others'

ds_numDone_cateDone = ds_numDone.withColumn('workclass_to_other', workclassToOther)\
              .withColumn('country_to_other', countryToOther)

# ds_numDone_cateDone = ds_numDone.withColumn('native.country', countryToOther)

# check whether the transformation is successful

ds_numDone.selectColumns(['native.country']).unique().show()

ds_numDone_cateDone.selectColumns(['country_to_other']).unique().show()

ds_numDone_cateDone.tocsv('incomeAfterTuplex.csv')

"""## ML Models"""

# ML
# random forest
# GBDT

incomeAfterTuplex = pd.read_csv('incomeAfterTuplex.part0.csv')

incomeAfterTuplex.head()

incomeAfterTuplex.describe(include = 'all')

# check missing values
for col in incomeAfterTuplex.columns:
  print("----------------")
  print("column name: %s count: %i, missing values: %i" % (col, incomeAfterTuplex[col].count(), incomeAfterTuplex[col].isnull().sum()))
  print("----------------")

# handle categorical data and numerical
# feature col
num_cols = []
cate_cols = []
for col,t in incomeAfterTuplex.dtypes.items():
  if t == 'object':
    if col != 'workclass' and col != 'native.country':
      cate_cols.append(col)
  else:
    if col != 'income>50K':
      num_cols.append(col)

incomeAfterTuplex[cate_cols]

"""## Handle Categorical Columns"""

# Initialize OneHotEncoder
encoder = OneHotEncoder(sparse=False, drop=None)

# Fit and transform data
encoded_data = encoder.fit_transform(incomeAfterTuplex[cate_cols])

# Convert to DataFrame
newCate_cols = encoder.get_feature_names_out()
encoded_df = pd.DataFrame(encoded_data, columns = newCate_cols)

# Concatenate with the original DataFrame and drop original categorical columns
incomeAfterTuplex = pd.concat([incomeAfterTuplex, encoded_df], axis=1).drop(cate_cols, axis=1)
incomeAfterTuplex.drop(['workclass', 'native.country'], axis=1, inplace = True)

incomeAfterTuplex[newCate_cols].head()

"""## Handle Numerical Columns"""

# standardize the nurmerical cols
scaler = StandardScaler()
incomeAfterTuplex[num_cols] = scaler.fit_transform(incomeAfterTuplex[num_cols])

incomeAfterTuplex[num_cols].head()

incomeAfterTuplex.head()

"""## Train Model"""

# Separate the features (X) and the target variable (y)
X = incomeAfterTuplex[newCate_cols.tolist() + num_cols]
y = incomeAfterTuplex['income>50K']

# Split the dataset into training (80%) and testing (20%) sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# random forest

# Train a random forest classifier
rf = RandomForestClassifier(n_estimators=100)
rf.fit(X_train, y_train)

# Make predictions
y_pred_rf = rf.predict(X_test)

# Calculate accuracy
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print("Random Forest Accuracy:", accuracy_rf)

# Evaluate the model
print(classification_report(y_test, y_pred_rf))

# GBDT

# Train a GBDT classifier
gbdt = GradientBoostingClassifier(n_estimators=200,learning_rate=0.05,max_depth=5)
gbdt.fit(X_train, y_train)

# Make predictions
y_pred_gbdt = gbdt.predict(X_test)

# Calculate accuracy
accuracy_gbdt = accuracy_score(y_test, y_pred_gbdt)
print("Random Forest Accuracy:", accuracy_gbdt)

# Evaluate the model
print(classification_report(y_test, y_pred_gbdt))